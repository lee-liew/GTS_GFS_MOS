{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuli/anaconda3/envs/MOS/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dtm\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import r2_score,explained_variance_score,mean_squared_error,mean_absolute_error,confusion_matrix\n",
    "from sklearn.linear_model import LassoCV,LinearRegression\n",
    "plt.rcParams['font.sans-serif']=[u'SimHei'] # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False  # 用来正常显示负号\n",
    "#显示所有列\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "#pd.set_option('display.max_rows', None)\n",
    "from config.base_config import * \n",
    "from config.models import *  #循环调用不同模型\n",
    "from config.model_config import * #几种模型的名称、参数设置\n",
    "from config.data_process_before_model import * #把数据划分为训练和测试，并把训练数据化为x和y\n",
    "from config.evaluation import * ##模型评估\n",
    "from config.Logger import * ##模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 876300 entries, 0 to 876299\n",
      "Data columns (total 5 columns):\n",
      "time    876300 non-null datetime64[ns]\n",
      "sta     876300 non-null int64\n",
      "lat     868800 non-null float64\n",
      "lon     868800 non-null float64\n",
      "t2m     868800 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(3), int64(1)\n",
      "memory usage: 33.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# GET multi_para OBS DATA 包含了所有国内站点的观测数据\n",
    "obs_txt_path='/home/liuli/GTS_GFS_MOS/domestic_hourly_data/'\n",
    "file1= pd.read_csv(obs_txt_path+'df_t2m.csv',sep=',')\n",
    "df1=pd.DataFrame(file1)\n",
    "df1['time']=pd.DatetimeIndex(df1['time'])\n",
    "print(df1.info()) #300站点，每天8个时次，365天，共（8*365+1）*300=876300行\n",
    "# get the station information\n",
    "file2= pd.read_csv(obs_txt_path+'6-para-station-info-t2m.txt', header=None, sep='\\t', names=['id','lat','lon'])\n",
    "df2=pd.DataFrame(file2)\n",
    "SID_TODO=list(df2['id'].astype('str'))\n",
    "#print(SID_TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Integrate the features into one file\n",
    "def make_feature_target(id):    \n",
    "    \"\"\"\n",
    "    This function is used for integrateing the splited feature files into a big one \n",
    "    and retrive the target data\n",
    "    \"\"\" \n",
    "    ###first we should judge if the file exist\n",
    "    count=0\n",
    "    for aa in range(1,13):\n",
    "        name_aa=id+'_multi_para_%02i.txt' % aa\n",
    "        if os.path.exists(fea_txt_path+'/'+name_aa):\n",
    "            count=count+1\n",
    "    if count==12:\n",
    "        print(\"%s target data is complete, now we will training the %s \" % (id,id) )\n",
    "        df_feature=pd.DataFrame(columns=feature_cols)\n",
    "        for cc in range(1,13):\n",
    "            name=id+'_multi_para_%02i.txt' % cc                \n",
    "            file_tmp= pd.read_csv(fea_txt_path+'/'+name, header=None, sep=' \\\\s+',names=feature_cols)\n",
    "            df_tmp=pd.DataFrame(file_tmp)\n",
    "            df_feature=pd.concat([df_feature,df_tmp],ignore_index=True) \n",
    "        start='2018-01-01 00:00:00'\n",
    "        end_of_db = '2019-01-01 00:00:00'\n",
    "        date_index = pd.date_range(start, end_of_db, freq = '3h')\n",
    "        date_df = pd.DataFrame(date_index,columns=['time'])\n",
    "        df_feature.insert(0,'time', date_df )#插入时间作为标记 \n",
    "        \n",
    "        df_target=df1[df1['sta'].isin([id])] #先挑出这个要素\n",
    "        df_target=df_target.sort_values(by='time')\n",
    "        df_target=df_target.reset_index(drop=True) \n",
    "        #print(df_target)\n",
    "        df_ready = pd.merge(df_feature,df_target, on=['time'], how='left')\n",
    "        return df_ready\n",
    "    else:\n",
    "        print(\"%s target data is not complete !\" % id)\n",
    "        return None\n",
    "    \n",
    "#for i_sid in SID_TODO:\n",
    "#    print(\"%s processing...\" % i_sid) \n",
    "#    # for each station ,there are 12 multi_para and 12 rain files\n",
    "#    df_ready=make_feature_target(i_sid)\n",
    "#    print(df_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_get_eval_df(df, model_out, target='TARGET'):\n",
    "    \"\"\"\n",
    "    merge target and prediction into one dataframe\n",
    "    \"\"\"\n",
    "    index_col_0 = ['time']\n",
    "#    if 'SID' in df.columns:\n",
    "#        index_col_0 = ['SID', 'datetime', 'datetime0']\n",
    "#    else:\n",
    "#        index_col_0 = ['datetime', 'datetime0']\n",
    "\n",
    "    X, y, _, index_remain, _ = prepare_training_data(\n",
    "        df, target=target, index_col=index_col_0)\n",
    "    \n",
    "    y_pred = model_out.predict(X)\n",
    "    # LGBM:\n",
    "    #     y_pred = model_out.predict(X, num_iteration=model_out.best_iteration)\n",
    "    y_arr = np.hstack((y.reshape(-1, 1), y_pred.reshape(-1, 1)))\n",
    "    df_pred = pd.DataFrame(\n",
    "        y_arr, index=index_remain, columns=['TARGET', 'OUR_FCST'])\n",
    "    df_pred = df_pred.reset_index()\n",
    "    col_nm_lst = index_col_0 + ['TARGET', 'OUR_FCST']\n",
    "    df_pred.columns = col_nm_lst\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_training_flow(df_0,\n",
    "                       df_test=None,\n",
    "                       target='t2m',\n",
    "                       sid='sid_test',\n",
    "                       model_config=None,\n",
    "                       pkl_model=False,\n",
    "                       model_dir=MODEL_DIR,\n",
    "                       gs=True,\n",
    "                       param_grid=None,\n",
    "                       rs=False,\n",
    "                       param_dist=None,\n",
    "                       printing=True,\n",
    "                       plotting=True,\n",
    "                       train_all=True):\n",
    "\n",
    "    model_name = model_config[0]\n",
    "    model_param = model_config[-1]\n",
    "    print(\"###### Training with %s ######\" % model_name)\n",
    "    if gs:\n",
    "        print(\"##### Training with grid_search. #####\")\n",
    "    else:\n",
    "        print(\"###### Params are: ######\")\n",
    "        print(model_param)\n",
    "    if train_all:\n",
    "        df_train = df_0\n",
    "    else:\n",
    "        # df_train, df_dev = train_test_df_spilt(df_0, ratio=0.7)\n",
    "        df_train, df_dev = train_test_df_spilt(\n",
    "            df_0,\n",
    "            ts='2018-11-01 00:00:00',\n",
    "            is_start=True,\n",
    "            date_col='time',\n",
    "            ratio=0.8,\n",
    "            split_method='r')\n",
    "        \n",
    "    index_col_0 = ['time']\n",
    "    #对应from mos_base.data_process_before_model import *里面的函数\n",
    "    ###主要是把dataframe分开成输入和target\n",
    "    X_train, y_train, features, index_remain_train, y_baseline = prepare_training_data(\n",
    "        df_train, target=target, index_col=index_col_0, date_col='time')\n",
    "\n",
    "    print(\">>>>> MODEL FLOW <<<<<\")\n",
    "    #from mos_base.models import *  #循环调用不同模型\n",
    "    model_out = func_model_flow(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        model_name=model_name,\n",
    "        model_param=model_param,\n",
    "        columns=features,\n",
    "        gs=gs,\n",
    "        param_grid=param_grid,\n",
    "        rs=rs,\n",
    "        param_dist=param_dist,\n",
    "        printing=printing,\n",
    "        plotting=plotting)\n",
    "\n",
    "    if pkl_model:\n",
    "        model_nm = \"%s_%s.model\" % (sid, model_name)\n",
    "        pkl_file_path = os.path.join(model_dir, model_nm)\n",
    "        print(\"========== Dumping Models ==========\")\n",
    "        with open(pkl_file_path, 'wb') as file:\n",
    "            pickle.dump(model_out, file)\n",
    "        print(\">>>>>>>>>> Finished !! <<<<<<<<<<\")\n",
    "\n",
    "    if train_all:\n",
    "        return None, None\n",
    "    else:\n",
    "        df_pred_dev = func_get_eval_df(df_dev, model_out, target=target)\n",
    "#        df_pred_test = None\n",
    "#        if isinstance(df_test, pd.DataFrame):\n",
    "#            df_pred_test = func_get_eval_df(df_test, model_out, target=target)\n",
    "#        return df_pred_dev, df_pred_test\n",
    "    return df_pred_dev  #return a df including target and predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_training(df_feature_ready,\n",
    "                     sid,\n",
    "                     pkl_model=True,\n",
    "                     model_dir=MODEL_DIR,\n",
    "                     plotting=False,\n",
    "                     printing=False,\n",
    "                     train_all=True):\n",
    "\n",
    "    result_metrics = {}\n",
    "#    dict_all_model_df = {}\n",
    "\n",
    "    for model_i in [ xgb_config,lgbm_config, et_config,  rf_config]:\n",
    "#    for model_i in [lgbm_config,et_config]:\n",
    "#        print( \"train_all\", train_all)\n",
    "        df_pred = func_training_flow(\n",
    "            df_feature_ready,\n",
    "            sid=sid,\n",
    "            pkl_model=pkl_model,\n",
    "            model_dir=model_dir,\n",
    "            model_config=model_i,\n",
    "            plotting=plotting,\n",
    "            printing=printing,\n",
    "            train_all=train_all)\n",
    "\n",
    "        if train_all:\n",
    "            continue\n",
    "        else:\n",
    "            print(\"start to evaluate\")\n",
    "            metrics_i = func_evaluation(df_pred['TARGET'], df_pred['OUR_FCST'])\n",
    "            result_metrics[model_i[0]] = metrics_i\n",
    "            \n",
    "#            if sid == 'ALL_SITES':\n",
    "#                dict_all_model_df[model_i[0]] = df_pred\n",
    "\n",
    "    if train_all: \n",
    "        return None,None\n",
    "    else:\n",
    "\n",
    "        print(result_metrics)\n",
    "        df_result_metrics = get_result_df(result_metrics)\n",
    "        return df_result_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def train_by_site(train_all=False):\n",
    "    \"\"\"\n",
    "    This function is used for training models SITE BY SITE, with their own feature csv.\n",
    "    \"\"\"\n",
    "    if train_all:\n",
    "        pkl_model = True\n",
    "        plotting = False\n",
    "    else:\n",
    "        pkl_model = False #是否保存模型结构\n",
    "        plotting = True #是否画出feature的重要性\n",
    "        \n",
    "    dict_result_metrics = {}\n",
    "\n",
    "    for i_sid in SID_TODO:\n",
    "        print(\"%s processing...\" % i_sid)\n",
    "        #首先读取特征和目标数据\n",
    "        df_ready=make_feature_target(i_sid)\n",
    "        if df_ready is None:\n",
    "            continue\n",
    "        #print(df_ready)\n",
    "        #开始跑模型\n",
    "        df_result_metrics= running_training(\n",
    "            df_ready,\n",
    "            sid=i_sid,\n",
    "            pkl_model=pkl_model,\n",
    "            model_dir=MODEL_DIR,\n",
    "            plotting=plotting,\n",
    "            printing=True,\n",
    "            train_all=train_all)\n",
    "        \n",
    "        dict_result_metrics[i_sid] = df_result_metrics\n",
    "\n",
    "    return dict_result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def train_all_site(train_all=False):\n",
    "    \"\"\"\n",
    "    This function is used for training models  BY ALL SITES.\n",
    "    \"\"\"\n",
    "    if train_all:\n",
    "        pkl_model = True\n",
    "        plotting = False\n",
    "    else:\n",
    "        pkl_model = False #是否保存模型结构\n",
    "        plotting = True #是否画出feature的重要性\n",
    "        \n",
    "    dict_result_metrics = {}\n",
    "    df_all=pd.DataFrame(columns=feature_cols)\n",
    "    \n",
    "    for i_sid in SID_TODO:\n",
    "        print(\"%s processing...\" % i_sid)\n",
    "        #首先读取特征和目标数据\n",
    "        df_ready=make_feature_target(i_sid)\n",
    "        df_all=pd.concat([df_all,df_ready],ignore_index=True)\n",
    "        #开始跑模型\n",
    "    #print(df_all)\n",
    "    df_result_metrics= running_training(\n",
    "            df_all,\n",
    "            sid=i_sid,\n",
    "            pkl_model=pkl_model,\n",
    "            model_dir=MODEL_DIR,\n",
    "            plotting=plotting,\n",
    "            printing=True,\n",
    "            train_all=train_all)\n",
    "        \n",
    "    dict_result_metrics= df_result_metrics\n",
    "\n",
    "    return dict_result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def train_ready_df(train_all=False):\n",
    "    \"\"\"\n",
    "    This function is used for training models  with a ready dataframe including target and .\n",
    "    \"\"\"\n",
    "    if train_all:\n",
    "        pkl_model = True\n",
    "        plotting = False\n",
    "    else:\n",
    "        pkl_model = False #是否保存模型结构\n",
    "        plotting = True #是否画出feature的重要性\n",
    "        \n",
    "    dict_result_metrics = {}\n",
    "    file_temp= pd.read_csv('/home/liuli/GTS_GFS_MOS/domestic_hourly_data/2287.csv',sep=',')\n",
    "    df_all=pd.DataFrame(file_temp)\n",
    "    #print(df_all)\n",
    "        #开始跑模型\n",
    "    i_sid=\"test\"\n",
    "    df_result_metrics= running_training(\n",
    "            df_all,\n",
    "            sid=i_sid,\n",
    "            pkl_model=pkl_model,\n",
    "            model_dir=MODEL_DIR,\n",
    "            plotting=plotting,\n",
    "            printing=True,\n",
    "            train_all=train_all)\n",
    "        \n",
    "    dict_result_metrics= df_result_metrics\n",
    "\n",
    "    return dict_result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['54594', '54824', '54815', '54819', '54820', '54821', '54822', '59213', '54825', '59235', '54828', '54829', '54831', '54833', '54834', '54836', '54814', '59134', '59133', '54812', '59130', '59127', '54809', '54808', '50246', '54805', '54803', '54778', '50434', '54777', '54774', '54765', '50468', '59230', '59238', '50516', '54936', '54919', '54920', '54923', '54927', '54929', '54932', '54939', '54841', '54940', '54943', '59033', '59025', '55576', '55585', '54916', '54915', '54912', '54911', '54910', '54908', '54905', '54901', '54861', '54857', '54852', '54851', '54848', '54846', '54844', '59246', '54842', '54764', '50525', '57060', '50862', '54609', '54613', '54615', '54617', '50873', '54618', '54621', '54644', '50859', '50853', '54624', '54626', '54631', '54640', '54607', '50888', '54606', '54605', '54603', '50924', '54597', '54590', '54575', '54565', '50946', '59058', '54541', '54539', '54534', '54533', '54531', '59052', '50834', '54759', '50618', '50656', '54726', '50647', '54731', '54734', '54736', '54738', '54645', '54744', '54749', '54751', '50548', '54752', '50531', '50659', '54723', '50739', '50742', '50745', '54713', '54710', '50758', '50767', '54709', '54703', '54701', '54662', '50787', '50788', '54660', '59125', '55589', '55591', '55593', '56856', '56839', '56840', '56846', '56849', '56851', '56854', '56863', '56882', '56869', '56870', '56871', '56875', '56876', '56878', '56838', '56833', '56783', '56781', '56774', '56767', '56757', '56755', '56751', '56742', '56739', '59254', '56693', '58944', '56691', '56684', '56654', '56881', '56885', '55598', '57014', '57001', '57002', '57003', '57006', '59264', '57008', '57016', '56886', '57028', '57029', '57037', '57039', '57052', '57053', '56987', '47035', '56985', '56984', '56977', '56976', '56970', '58942', '56964', '56959', '56954', '56948', '56946', '56944', '56898', '56891', '56889', '56652', '56646', '56598', '56183', '56125', '56128', '56137', '56172', '56180', '56182', '56184', '56595', '56187', '56190', '56199', '56223', '56247', '56257', '56096', '56093', '56091', '56081', '56080', '56074', '56071', '56065', '56046', '56043', '56033', '56029', '56016', '56004', '59012', '55773', '55655', '56284', '56288', '56289', '56474', '56586', '56584', '56580', '58946', '56533', '56497', '56496', '56494', '56490', '56487', '56485', '56483', '56478', '56475', '56441', '56295', '58954', '56399', '56396', '56393', '56391', '56389', '56387', '56386', '56383', '56382', '56357', '56307', '56298', '56297', '54528', '50964', '54523', '53841', '53792', '53796', '53798', '53799', '53817', '59096', '53850', '53869', '52896', '53853', '52889', '53857', '52884', '53865', '53789', '53788', '53785', '53782', '52974']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sys.stdout = Logger(\"./log_gpu.txt\")  \n",
    "\n",
    "    print(SID_TODO)\n",
    "    \n",
    "    data_final=train_ready_df(train_all=False)\n",
    "    print(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
